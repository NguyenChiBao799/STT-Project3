# rtc_integration_layer.py
import asyncio
import os
from pathlib import Path
from typing import AsyncGenerator, Callable, Optional, Tuple, Any, Dict, ClassVar
from datetime import datetime as _dt
import numpy as np 
import torch 
import traceback 
import time 
import base64
import httpx 
import whisper # C·∫ßn c√†i ƒë·∫∑t th∆∞ vi·ªán Whisper
from concurrent.futures import ThreadPoolExecutor
# Th√™m import cho GTTS v√† chuy·ªÉn ƒë·ªïi audio
import io 
import wave
try:
    from gtts import gTTS
    from pydub import AudioSegment 
    GTTS_IS_READY = True
except ImportError:
    gTTS = None 
    AudioSegment = None
    GTTS_IS_READY = False


# --- C·∫•u h√¨nh API N·ªôi b·ªô ---
INTERNAL_UPLOAD_URL = "http://internal.company.api/v1/voice_logs/upload" 
INTERNAL_API_KEY = "YOUR_DEFAULT_INTERNAL_API_KEY_HERE" 
# ----------------------------------------------------------------------

# --- SAFE IMPORTS (CONFIG, DIALOG MANAGER V√Ä RESPONSE GENERATOR) ---
try:
    from config_db import WHISPER_MODEL_NAME, SAMPLE_RATE 
    from dialog_manager import DialogManager 
    from response_generator import ResponseGenerator # ResponseGenerator ƒë∆∞·ª£c DialogManager s·ª≠ d·ª•ng
except ImportError:
    # Fallback/Mock n·∫øu kh√¥ng t√¨m th·∫•y c√°c l·ªõp c·ªët l√µi
    WHISPER_MODEL_NAME = "tiny" 
    SAMPLE_RATE = 16000
    class ResponseGenerator:
        def __init__(self, *args, **kwargs): pass
    class DialogManager:
        def __init__(self, *args, **kwargs): pass
        def process_audio_file(self, record_file, user_input_asr): 
            res_text = f"L·ªñI DM: Kh√¥ng t√¨m th·∫•y DialogManager. ASR: {user_input_asr}"
            if "[NO SPEECH DETECTED]" in user_input_asr:
                 res_text = "Xin l·ªói, t√¥i kh√¥ng nghe r√µ. B·∫°n c√≥ th·ªÉ n√≥i l·∫°i kh√¥ng?"
            return {"response_text": res_text, "response_audio_path": None, "user_input_asr": user_input_asr}


RECORDING_DIR = Path("rtc_recordings"); RECORDING_DIR.mkdir(exist_ok=True) 

def _log_colored(message, color="white"):
    color_map = {
        "red": "\033[91m", "green": "\033[92m", "yellow": "\033[93m", 
        "blue": "\033[94m", "magenta": "\033[95m", "cyan": "\033[96m", "white": "\033[97m", "orange": "\033[33m"
    }
    RESET = "\033[0m"
    print(f"{color_map.get(color, RESET)}{message}{RESET}", flush=True)


# ==================== VAD/ASR LOGIC ====================
WHISPER_IS_READY = False
try:
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    USE_FP16 = (DEVICE == "cuda") 
    
    # T·∫£i VAD (Silero)
    VAD_MODEL, VAD_UTILS = torch.hub.load(
        repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False, onnx=False, trust_repo=True 
    )
    VAD_MODEL = VAD_MODEL.to(DEVICE)
    (get_speech_timestamps, save_audio, read_audio, VAD_collect_chunks, *vad_extra_utils) = VAD_UTILS 
    
    # T·∫£i Whisper
    WHISPER_MODEL = whisper.load_model(WHISPER_MODEL_NAME)
    if USE_FP16: WHISPER_MODEL = WHISPER_MODEL.half()
    WHISPER_MODEL.to(DEVICE)
    WHISPER_IS_READY = True
except Exception as e:
    DEVICE = "cpu"
    WHISPER_MODEL = None
    _log_colored(f"‚ùå L·ªói kh·ªüi t·∫°o ASR/VAD (Whisper/Torch): {e}", "red")

def _apply_silero_vad(audio_filepath: Path, log_callback: Callable) -> Optional[np.ndarray]:
    """√Åp d·ª•ng VAD ƒë·ªÉ lo·∫°i b·ªè kho·∫£ng l·∫∑ng."""
    if not WHISPER_IS_READY: return None
    try:
        audio_numpy = whisper.load_audio(str(audio_filepath))
        audio_tensor = torch.from_numpy(audio_numpy).float()
        speech_timestamps = get_speech_timestamps(audio_tensor.to(DEVICE), VAD_MODEL, sampling_rate=SAMPLE_RATE, threshold=0.3)
        if not speech_timestamps: return None 
        speech_audio_tensor = VAD_collect_chunks(speech_timestamps, audio_tensor)
        speech_audio_numpy = speech_audio_tensor.cpu().numpy()
        MIN_SPEECH_DURATION_SECONDS = 0.5
        filtered_duration = len(speech_audio_numpy) / SAMPLE_RATE
        if filtered_duration < MIN_SPEECH_DURATION_SECONDS: return None 
        return speech_audio_numpy
    except Exception:
        return whisper.load_audio(str(audio_filepath))

class ASRServiceWhisper:
    def __init__(self, log_callback: Callable, model):
        self._log = log_callback 
        self.model = model
    async def transcribe(self, audio_filepath: Path) -> AsyncGenerator[str, None]:
        if not WHISPER_IS_READY: yield ""; return
        try:
            audio_input = await asyncio.to_thread(_apply_silero_vad, audio_filepath, self._log)
            if audio_input is None: yield "[NO SPEECH DETECTED]"; return
            result = await asyncio.to_thread(self.model.transcribe, audio_input, language="vi", fp16=USE_FP16)
            yield result.get("text", "").strip()
        except Exception as e:
            self._log(f"‚ùå [ASR] L·ªñI WHISPER: {e}", "red")
            yield "" 

# ==================== D·ªäCH V·ª§ UPLOAD AUDIO ====================

async def _upload_audio_to_internal_api(file_path: Path, session_id: str, log_callback: Callable, api_key: str = INTERNAL_API_KEY):
    """Gi·∫£ l·∫≠p/th·ª±c hi·ªán upload file audio l√™n API n·ªôi b·ªô."""
    if str(INTERNAL_UPLOAD_URL).startswith("http://internal.company.api"):
        log_callback("‚ö†Ô∏è [UPLOAD] B·ªè qua upload: URL v·∫´n l√† placeholder.", "orange")
        return False
        
    try:
        log_callback(f"[{_dt.now().strftime('%H:%M:%S')}] üì§ [UPLOAD] B·∫Øt ƒë·∫ßu upload file: {file_path.name}...", "yellow")
        async with httpx.AsyncClient(verify=False, timeout=30.0) as client: 
            with open(file_path, 'rb') as f:
                files = {'file': (file_path.name, f, 'audio/wav')}
                headers = {'X-API-Key': api_key, 'X-Session-ID': session_id} 
                response = await client.post(INTERNAL_UPLOAD_URL, files=files, headers=headers)
                response.raise_for_status() 
                log_callback(f"[{_dt.now().strftime('%H:%M:%S')}] ‚úÖ [UPLOAD] Upload th√†nh c√¥ng!", "green")
                return True
    except Exception as e:
        log_callback(f"[{_dt.now().strftime('%H:%M:%S')}] ‚ùå [UPLOAD] L·ªñI UPLOAD: {e}", "red")
        return False

# ==================== D·ªäCH V·ª§ TTS (GTTS Streaming) ====================

class TTSServiceGTTS:
    """S·ª≠ d·ª•ng th∆∞ vi·ªán gTTS ƒë·ªÉ t·∫°o audio MP3 v√† chuy·ªÉn ƒë·ªïi sang PCM 16kHz ƒë·ªÉ streaming."""
    
    TTS_LANG: ClassVar[str] = "vi" 
    
    def __init__(self, log_callback: Callable): 
        self._log = log_callback
        self._is_ready = GTTS_IS_READY
        
        if not self._is_ready:
            self._log("‚ö†Ô∏è [TTS] Th∆∞ vi·ªán gTTS ho·∫∑c pydub kh√¥ng s·∫µn s√†ng. S·∫Ω d√πng Fallback Mock.", "orange")
        else:
            self._log("‚úÖ [TTS] D·ªãch v·ª• gTTS s·∫µn s√†ng.", "green")

    def _synthesize_blocking(self, text: str) -> Optional[bytes]:
        """H√†m ƒë·ªìng b·ªô (Blocking) ƒë·ªÉ g·ªçi gTTS, t·∫°o MP3, v√† chuy·ªÉn ƒë·ªïi sang WAV/PCM 16kHz."""
        if not self._is_ready:
             return None 
             
        self._log(f"üß† [GTTS] B·∫Øt ƒë·∫ßu t·ªïng h·ª£p vƒÉn b·∫£n: '{text[:30]}...'", "magenta")
        
        try:
            # 1. T·∫°o audio MP3 b·∫±ng gTTS (output stream)
            tts = gTTS(text=text, lang=self.TTS_LANG)
            mp3_buffer = io.BytesIO()
            tts.write_to_fp(mp3_buffer)
            mp3_buffer.seek(0)
            
            # 2. T·∫£i MP3 v√† chuy·ªÉn ƒë·ªïi sang PCM 16kHz, 16-bit, Mono (D√πng pydub, c·∫ßn FFmpeg)
            audio = AudioSegment.from_file(mp3_buffer, format="mp3")
            
            # √Åp d·ª•ng c√°c thay ƒë·ªïi c·∫ßn thi·∫øt cho WebRTC:
            audio = audio.set_channels(1) # Mono
            audio = audio.set_frame_rate(SAMPLE_RATE) # 16000 Hz
            audio = audio.set_sample_width(2) # 16-bit (2 bytes)
            
            # 3. Ghi AudioSegment sang ƒë·ªãnh d·∫°ng WAV ƒë·ªÉ d·ªÖ d√†ng tr√≠ch xu·∫•t PCM
            pcm_buffer = io.BytesIO()
            audio.export(pcm_buffer, format="wav") 
            
            # Tr·∫£ v·ªÅ to√†n b·ªô n·ªôi dung WAV (bao g·ªìm header 44 bytes)
            wav_data = pcm_buffer.getvalue()
            
            self._log(f"üéµ [GTTS] ƒê√£ t·∫°o v√† chuy·ªÉn ƒë·ªïi audio WAV/PCM {len(wav_data)} bytes.", "magenta")
            return wav_data

        except Exception as e:
            self._log(f"‚ùå [GTTS] L·ªói khi t·∫°o/chuy·ªÉn ƒë·ªïi audio gTTS (Ki·ªÉm tra FFmpeg/Pydub): {e}", "red")
            self._log(traceback.format_exc(), "red") 
            return None

    async def synthesize_stream(self, text: str) -> AsyncGenerator[bytes, None]:
        self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üéµ [TTS] B·∫Øt ƒë·∫ßu t·ªïng h·ª£p √¢m thanh...", "magenta")
        
        # Ch·∫°y t√°c v·ª• blocking trong Thread Pool
        audio_data_bytes = await asyncio.get_event_loop().run_in_executor(
            None, 
            self._synthesize_blocking,
            text
        )
        
        if audio_data_bytes is None or len(audio_data_bytes) <= 44:
             # Fallback Mock: 2 gi√¢y PCM 16kHz (32000 bytes)
             self._log("‚ö†Ô∏è [TTS MOCK] M√¥ h√¨nh gTTS l·ªói. S·ª≠ d·ª•ng audio chunk gi·∫£ l·∫≠p (Base64 random).", "orange")
             audio_data_bytes = os.urandom(32000) 
             PCM_DATA_OFFSET = 0 # N·∫øu l√† mock, kh√¥ng c·∫ßn offset
        else:
             PCM_DATA_OFFSET = 44 # N·∫øu l√† WAV, b·ªè qua 44 byte WAV header
             
        # 2. CHIA CHUNK V√Ä STREAM (B·∫•t ƒë·ªìng b·ªô)
        CHUNK_SIZE_BYTES = 1600 
        
        streamable_data = audio_data_bytes[PCM_DATA_OFFSET:]
        
        for i in range(0, len(streamable_data), CHUNK_SIZE_BYTES):
            chunk = streamable_data[i:i + CHUNK_SIZE_BYTES]
            if not chunk: continue
            
            base64_chunk = base64.b64encode(chunk) 
            yield base64_chunk
            
            await asyncio.sleep(0.01) # Gi·∫£ l·∫≠p ƒë·ªô tr·ªÖ streaming (10ms)
            
        self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üéµ [TTS] K·∫øt th√∫c lu·ªìng audio TTS.", "magenta")

# ==================== L·ªöP X·ª¨ L√ù RTC T√çCH H·ª¢P M·ªöI (ƒê√£ s·ª≠a ƒë·ªïi) ====================

class RTCStreamProcessor:
    
    def __init__(self, log_callback: Optional[Callable] = None):
        # ƒê·∫£m b·∫£o s·ª≠ d·ª•ng _log
        self._log = log_callback if log_callback else _log_colored
        self._asr_client = ASRServiceWhisper(self._log, WHISPER_MODEL) if WHISPER_IS_READY else type('ASRMock', (object,), {'transcribe': lambda self, fp: (yield "Transcript gi·∫£ l·∫≠p.")})()
        
        # ‚úÖ S·ª¨ D·ª§NG TTSServiceGTTS
        self._tts_client = TTSServiceGTTS(self._log)
        
        # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ ch·∫°y c√°c t√°c v·ª• ƒë·ªìng b·ªô (DM)
        self._executor = ThreadPoolExecutor(max_workers=1)
    
    async def handle_rtc_session(self, 
                                 record_file: Path,
                                 session_id: str,
                                 api_key: str) \
                                 -> AsyncGenerator[Tuple[bool, Any], None]:
        
        self._log(f"‚ñ∂Ô∏è [RTC] B·∫Øt ƒë·∫ßu phi√™n x·ª≠ l√Ω ASR/NLU. Session ID: {session_id}.", "cyan") 
        full_transcript = ""
        response_text = "Xin l·ªói, t√¥i ch∆∞a th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu."
        
        try: 
            # KH·ªûI T·∫†O DIALOG MANAGER V·ªöI API KEY
            dm_instance = DialogManager(log_callback=self._log, mode="RTC", api_key=api_key) 
            
            yield (False, {"type": "generator_init", "user_text": "", "bot_text": ""}) 
            
            # 1. UPLOAD AUDIO (B·∫•t ƒë·ªìng b·ªô)
            await _upload_audio_to_internal_api(record_file, session_id, self._log, api_key)
            
            # 2. [ASR Engine] (B·∫•t ƒë·ªìng b·ªô)
            asr_stream = self._asr_client.transcribe(record_file)
            async for partial_text in asr_stream:
                 if partial_text: full_transcript = partial_text
                     
            dm_input_asr = full_transcript.strip() if full_transcript.strip() and partial_text != "[NO SPEECH DETECTED]" else "[NO SPEECH DETECTED]"
            
            # 3-5. [Dialog Manager] (ƒê·ªìng b·ªô)
            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üß† [DM/NLU] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω DialogManager...", "yellow")
            
            # S·ª¨A L·ªñI 1: Thay keyword argument th√†nh positional argument
            dm_result = await asyncio.get_event_loop().run_in_executor(
                 self._executor,
                 dm_instance.process_audio_file, 
                 str(record_file), 
                 dm_input_asr # <--- POSITIONAL ARGUMENT
            )
            response_text = dm_result.get("response_text", response_text)

            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üß† [DM] Ho√†n t·∫•t. Response: '{response_text[:50]}...'", "green")

            yield (False, {"user_text": full_transcript.strip(), "bot_text": response_text})

            # 6. [TTS Engine] -> [Speaker Output] (Stream)
            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] üéµ [TTS] B·∫Øt ƒë·∫ßu streaming audio ph·∫£n h·ªìi...", "magenta")
            tts_audio_stream = self._tts_client.synthesize_stream(response_text)
            async for audio_chunk in tts_audio_stream:
                yield (True, audio_chunk)
        
        except asyncio.CancelledError:
            raise
        except Exception as e:
            self._log(f"[{_dt.now().strftime('%H:%M:%S')}] ‚ùå [RTC] L·ªñI X·ª¨ L√ù CHUNG: {e}", "red")
            # S·ª¨A L·ªñI 2: self.log -> self._log
            self._log(traceback.format_exc(), "red") 
            yield (False, {"type": "error", "user_text": full_transcript.strip(), "bot_text": f"L·ªói h·ªá th·ªëng: {e}"})
        finally: 
             self._log(f"[{_dt.now().strftime('%H:%M:%S')}] [RTC] K·∫øt th√∫c x·ª≠ l√Ω RTC.", "cyan")