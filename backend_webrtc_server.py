print("‚ö° RUNNING BACKEND FILE:", __file__)
import asyncio
import os
import json
import uuid
import wave
import numpy as np
from scipy.signal import resample_poly 
import warnings
from typing import Dict, Any, Optional, Callable
from pathlib import Path
import traceback 
from fastapi import FastAPI, Request, UploadFile, File, Form
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from aiortc import RTCPeerConnection, RTCSessionDescription, MediaStreamTrack, RTCConfiguration, RTCIceServer
from aiortc.exceptions import InvalidStateError

# Routers
from routers import products, orders, promotions, payment

# WebRTC Pipeline
from ai_modules.rtc_integration_layer import RTCStreamProcessor, SAMPLE_RATE, INTERNAL_API_KEY

# === MODULES M·ªöI (NLU ‚Üí LOGIC ‚Üí DIALOG) ===
from core.logic_manager import LogicManager
from ai_modules.dialog_manager import DialogManager
from core.stt_log_parser import STTLogParser
from core.json_loader import JSONLogLoader

import base64
from gtts import gTTS

# ============================================================
# üîß FIX CHO L·ªñI "Transaction.__retry()" TRONG AIORTC/AIOICE
# ============================================================
import aioice
aioice.stun.TRANSACTION_RETRY_INTERVAL = 2.0
aioice.stun.TRANSACTION_MAX_RETRIES = 4

warnings.filterwarnings("ignore", category=RuntimeWarning, message="invalid state")
warnings.filterwarnings("ignore", category=RuntimeWarning, message="coroutine.*was never awaited")

# ============================================================
# C·∫§U H√åNH CHUNG
# ============================================================
CHANNELS = 1
SAMPLE_WIDTH = 2
os.makedirs("temp", exist_ok=True)

ICE_SERVERS = [
    {"urls": "stun:stun1.l.google.com:19302"},
    {"urls": "stun:stun2.l.google.com:19302"},
    {"urls": "stun:stun3.l.google.com:19302"},
]

processing_tasks: Dict[str, asyncio.Task] = {}

# ============================================================
# APP KH·ªûI T·∫†O
# ============================================================
app = FastAPI(title="STT Voice AI Backend (WebRTC + REST API)")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(products.router, prefix="/api")
app.include_router(orders.router, prefix="/api")
app.include_router(promotions.router, prefix="/api")
app.include_router(payment.router, prefix="/api")


# ============================================================
# LOGGING CHU·∫®N HO√Å (D√ôNG CHO C·∫¢ LOGICMANAGER & DM)
# ============================================================
def log_info(message: str, color="white"):
    print(f"INFO:backend_webrtc_server:[{message}]")
from core.memory_trainer import MemoryTrainer
memory_engine = MemoryTrainer(log_callback=log_info)


# üî• T√çCH H·ª¢P LOGIC MANAGER + DIALOG MANAGER (ƒê√öNG API KEY)
logic_manager = LogicManager(
    log_callback=log_info,
    response_config={},        # ho·∫∑c load file config n·∫øu b·∫°n c√≥
    llm_mode="real",
    tts_mode="real",
    db_mode="real",
    api_key=INTERNAL_API_KEY   # <-- FIX API KEY
)

dialog_manager = DialogManager(
    log_callback=log_info,
    mode="rtc"                 # n·∫øu DM c·ªßa b·∫°n c·∫ßn mode
)


# ============================================================
# UTILITIES
# ============================================================
def _write_wav_file_safe_helper(file_path_str: str, chunks: list[bytes], wav_params_tuple: tuple):
    with wave.open(file_path_str, 'wb') as wf:
        wf.setparams(wav_params_tuple)
        for chunk in chunks:
            wf.writeframes(chunk)
    log_info(f"[WAV Writer] ‚úÖ Ghi file th√†nh c√¥ng: {file_path_str}")

WAV_PARAMS = (CHANNELS, SAMPLE_WIDTH, SAMPLE_RATE, 0, 'NONE', 'not compressed')

# ============================================================
# CLASS GHI √ÇM AUDIO
# ============================================================
class AudioFileRecorder:
    def __init__(self, pc):
        self._pc = pc
        self._on_stop_callback: Optional[Callable] = None
        self._track: Optional[MediaStreamTrack] = None
        self._file_path: Optional[Path] = None
        self._stop_event = asyncio.Event()
        self._chunks: list[bytes] = []
        self._record_task: Optional[asyncio.Task] = None 

    def start(self, track: MediaStreamTrack, file_path: str):
        self._track = track
        self._file_path = Path(file_path)
        self._stop_event.clear()
        self._chunks = []
        self._record_task = asyncio.create_task(self._read_track_and_write()) 
        log_info(f"[Recorder] ‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu ghi √¢m: {self._file_path.name}")

    def on(self, event: str, callback: Callable):
        if event == "stop":
            self._on_stop_callback = callback

    async def _read_track_and_write(self):
        try:
            while not self._stop_event.is_set():
                try:
                    packet = await self._track.recv()
                    audio_data_np = packet.to_ndarray()

                    # Chu·∫©n ho√° dtype
                    if audio_data_np.dtype == np.float32:
                        audio_data_np = (audio_data_np * 32767).astype(np.int16)
                    elif audio_data_np.dtype != np.int16:
                        audio_data_np = audio_data_np.astype(np.int16)

                    # Convert stereo ‚Üí mono
                    if len(audio_data_np.shape) > 1:
                        audio_data_np = np.mean(audio_data_np, axis=1).astype(np.int16)

                    # üöÄ RESAMPLE REAL-TIME KH√îNG BLOCKING
                    # 48k ‚Üí 16k d√πng polyphase filter (si√™u nhanh)
                    audio_data_np = resample_poly(audio_data_np, 1, 3).astype(np.int16)

                    # L∆∞u chunk
                    self._chunks.append(audio_data_np.tobytes())

                except InvalidStateError:
                    break
                except Exception as e:
                    if not self._stop_event.is_set():
                        log_info(f"[Recorder] L·ªói nh·∫≠n packet audio: {e}")
                    break

        except asyncio.CancelledError:
            log_info(f"[Recorder] üõë Task ƒë·ªçc track b·ªã h·ªßy.")

        finally:
            if not self._chunks:
                if self._on_stop_callback and self._file_path:
                    self._on_stop_callback(None)
                return

            try:
                # Ghi WAV chu·∫©n
                await asyncio.to_thread(
                    _write_wav_file_safe_helper,
                    str(self._file_path),
                    self._chunks,
                    WAV_PARAMS
                )

                if self._on_stop_callback:
                    self._on_stop_callback(str(self._file_path))

            except Exception as e:
                log_info(f"[Recorder] ‚ùå L·ªói ghi file WAV: {e}")
                if self._on_stop_callback:
                    self._on_stop_callback(None)


    def stop(self):
        log_info("[Recorder] üõë D·ª´ng ghi √¢m.")
        self._stop_event.set()
        if self._record_task:
            self._record_task.cancel()

# ============================================================
# H√ÄM X·ª¨ L√ù AUDIO SAU GHI
# ============================================================
async def _process_audio_and_respond(session_id, dm_processor, pc, data_channel, record_file, api_key):
    try:
        if not record_file or not os.path.exists(record_file):
            if data_channel:
                data_channel.send(json.dumps({
                    "type": "error",
                    "error": "Kh√¥ng c√≥ d·ªØ li·ªáu audio ho·∫∑c file kh√¥ng t·ªìn t·∫°i."
                }))
            log_info(f"[{session_id}] ‚ö†Ô∏è B·ªè qua: file audio None ho·∫∑c kh√¥ng t·ªìn t·∫°i.")
            return

        # === B·∫ÆT ƒê·∫¶U PIPELINE ===
        stream_generator = dm_processor.handle_rtc_session(
            record_file=Path(record_file),
            session_id=session_id,
            api_key=api_key
        )

        # === DATA GI·ªÆ L·∫†I ƒê·ªÇ T·ªîNG H·ª¢P CU·ªêI ===
        audio_chunks_binary = []
        last_user_text = ""
        last_bot_text = ""
        last_intent = ""
        last_action = ""
        last_payment_url = None

        parser = STTLogParser(log_callback=log_info)

        # ===========================
        #   V√íNG L·∫∂P NH·∫¨N STREAM
        # ===========================
        async for is_audio, data in stream_generator:

            # --- AUDIO STREAM ---
            if is_audio:
                audio_chunks_binary.append(
                    base64.b64decode(data) if isinstance(data, str) else data
                )
                continue

            # --- TEXT STREAM ASR ---
            if "user_text" in data and data["user_text"].strip():
                last_user_text = data["user_text"].strip()

            # --- PARSER ---
            nlu_json = parser.convert({"text_response": {"user_text": last_user_text}})

            # --- LOGIC MANAGER ---
            decision = logic_manager.handle_nlu_result(nlu_json)
            last_intent = decision.get("intent")
            last_action = decision.get("action")
            last_payment_url = decision.get("payment_url")

            # --- DIALOG MANAGER ---
            final_response = dialog_manager.process_with_logic_manager(
                nlu_json=nlu_json,
                logic_manager=logic_manager
            )

            # L·∫•y bot_text n·∫øu c√≥
            bot_raw = final_response.get("response_text") or final_response.get("text")
            if bot_raw and bot_raw.strip():
                last_bot_text = bot_raw.strip()

            # G·ª≠i TEXT PARTIAL cho UI
            if data_channel:
                data_channel.send(json.dumps({
                    "type": "text_response_partial",
                    "user_text": last_user_text,
                    "bot_text": last_bot_text,
                    "intent": last_intent,
                    "action": last_action,
                    "payment_url": last_payment_url
                }))

        # ===========================
        #   TTS SAU KHI K·∫æT TH√öC
        # ===========================
        output_file_name = f"{session_id}_output.wav"
        output_file_path = os.path.join("temp", output_file_name)

        user_spoken = last_user_text if last_user_text else "t√¥i kh√¥ng nghe r√µ c√¢u b·∫°n n√≥i"
        bot_spoken = last_bot_text if last_bot_text else "T√¥i xin l·ªói, hi·ªán t·∫°i t√¥i ch∆∞a t·∫°o ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi."

        tts_text = f"B·∫°n v·ª´a n√≥i: {user_spoken}. C√¢u tr·∫£ l·ªùi c·ªßa t√¥i l√†: {bot_spoken}."

        from pydub import AudioSegment

        log_info(f"[üß† [GTTS]] T·ªïng h·ª£p vƒÉn b·∫£n FULL: '{tts_text[:80]}...'")

        mp3_path = os.path.join("temp", f"{session_id}_tts.mp3")
        wav_path = os.path.join("temp", f"{session_id}_output.wav")

        # T·∫°o MP3 tr∆∞·ªõc
        gTTS(tts_text, lang='vi').save(mp3_path)

        # Convert MP3 ‚Üí WAV chu·∫©n PCM16 16kHz mono
        audio = AudioSegment.from_mp3(mp3_path)
        audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)
        audio.export(wav_path, format="wav")

        output_file_path = wav_path

        log_info(f"[üéµ [GTTS]] ƒê√£ t·∫°o file WAV PCM16 ƒë·∫ßy ƒë·ªß: {wav_path}")

        # ===========================
        #  GHI LOG JSON
        # ===========================
        response_json_path = os.path.join("temp", f"{session_id}_response.json")
        with open(response_json_path, "w", encoding="utf-8") as jf:
            json.dump({
                "session_id": session_id,
                "input_file": record_file,
                "output_audio": output_file_path,
                "user_text": user_spoken,
                "bot_text": bot_spoken,
                "intent": last_intent,
                "action": last_action,
                "payment_url": last_payment_url
            }, jf, ensure_ascii=False, indent=4)
            memory_engine.remember(response_json_path)
            memory_engine.build_intent_dataset()
            memory_engine.train_intent_classifier()


        # ===========================
        #  G·ª¨I EVENT END SESSION
        # ===========================
        if data_channel:
            data_channel.send(json.dumps({
                "type": "end_of_session",
                "bot_audio_path": f"/audio_files/{output_file_name}"
            }))

        log_info(f"[{session_id}] ‚úÖ Ho√†n t·∫•t. Audio ƒë·∫ßy ƒë·ªß g·ª≠i v·ªÅ client.")

    except Exception as e:
        log_info(f"[{session_id}] ‚ùå L·ªói x·ª≠ l√Ω audio: {e}")
        traceback.print_exc()

# ============================================================
# ENDPOINT /offer ‚Äî FULL CODE ƒê√É T√çCH H·ª¢P M·ªöI
# ============================================================
@app.post("/offer")
async def offer(request: Request):
    params = await request.json()

    # WebRTC Offer t·ª´ client
    offer = RTCSessionDescription(sdp=params["sdp"], type=params["type"])

    # Session ID t·ª´ client ho·∫∑c t·ª± sinh
    session_id = params.get("session_id", str(uuid.uuid4()))

    # API Key (b·∫±ng key n·ªôi b·ªô tr√™n backend)
    api_key = params.get("api_key", INTERNAL_API_KEY)

    logic_manager.api_key = api_key
    dialog_manager.api_key = api_key


    # =======================
    # T·∫°o c·∫•u h√¨nh WebRTC ICE
    # =======================
    config = RTCConfiguration(
        iceServers=[RTCIceServer(urls=s["urls"]) for s in ICE_SERVERS]
    )
    pc = RTCPeerConnection(configuration=config)

    # Recorder ‚Äî nh·∫≠n track audio t·ª´ client
    recorder = AudioFileRecorder(pc)

    # DataChannel holder
    data_channel_holder = None

    # ==============================================================
    # Khi client m·ªü DataChannel ‚Üí gi·ªØ reference ƒë·ªÉ g·ª≠i text_response
    # ==============================================================
    @pc.on("datachannel")
    def on_datachannel(ch):
        nonlocal data_channel_holder
        data_channel_holder = ch
        log_info(f"[{session_id}] üì° DataChannel nh·∫≠n: {ch.label}")

        @ch.on("message")
        async def handle_message(message):
            try:
                # N·∫øu l√† binary (PCM t·ª´ AudioWorklet) ‚Üí b·ªè qua kh√¥ng parse
                if isinstance(message, (bytes, bytearray, memoryview)):
                    return

                # N·∫øu kh√¥ng ph·∫£i string ‚Üí b·ªè qua
                if not isinstance(message, str):
                    return

                # N·∫øu l√† JSON th·∫≠t ‚Üí x·ª≠ l√Ω b√¨nh th∆∞·ªùng
                data = json.loads(message)

                if data.get("type") == "stop_recording":
                    log_info(f"[{session_id}] üõë Nh·∫≠n y√™u c·∫ßu STOP RECORDING t·ª´ client")
                    recorder.stop()
                    await asyncio.sleep(0)

            except Exception as e:
                # Ch·ªâ log l·ªói n·∫øu message l√† string JSON
                if isinstance(message, str):
                    log_info(f"[{session_id}] ‚ùå L·ªói message handler: {e}")
        return



    # ==============================================================
    # Khi client g·ª≠i audio track
    # ==============================================================
    @pc.on("track")
    def on_track(track):
        log_info(f"[{session_id}] üé§ Nh·∫≠n track audio: {track.kind}")

        if track.kind == "audio":
            path = os.path.join("temp", f"{session_id}_input.wav")

            # B·∫Øt ƒë·∫ßu ghi file WAV t·ª´ audio track
            recorder.start(track, path)

            # X·ª≠ l√Ω khi recorder d·ª´ng (g·ª≠i v√†o pipeline)
            recorder.on(
                "stop",
                lambda file_path: asyncio.create_task(
                    _process_audio_and_respond(
                        session_id=session_id,
                        dm_processor=RTCStreamProcessor(log_callback=log_info),
                        pc=pc,
                        data_channel=data_channel_holder,
                        record_file=file_path,
                        api_key=api_key
                    )
                )
            )

    # ==============================================================
    # SETUP OFFER ‚Äî TR·∫¢ ANSWER CHO CLIENT
    # ==============================================================
    await pc.setRemoteDescription(offer)
    answer = await pc.createAnswer()
    await pc.setLocalDescription(answer)

    return {
        "sdp": pc.localDescription.sdp,
        "type": pc.localDescription.type,
        "session_id": session_id
    }
# ============================================================
# üìÇ ENDPOINT: UPLOAD WAV FILE (D√ôNG CHO TEST & DEBUG)
# ============================================================
@app.post("/api/upload_wav")
async def upload_wav(file: UploadFile = File(...), api_key: str = Form(None)):
    """
    üìÇ Endpoint: T·∫£i file WAV l√™n backend ƒë·ªÉ ph√¢n t√≠ch:
    ‚Üí STT ‚Üí Parser ‚Üí LogicManager ‚Üí DialogManager ‚Üí Bot_text ‚Üí Bot_audio
    """
    try:
        os.makedirs("temp", exist_ok=True)
        session_id = str(uuid.uuid4())

        # --------------------------------------------------------
        # 1) L∆∞u file WAV ƒë∆∞·ª£c upload v√†o th∆∞ m·ª•c temp
        # --------------------------------------------------------
        temp_path = os.path.join("temp", f"{session_id}_uploaded.wav")
        with open(temp_path, "wb") as f:
            f.write(await file.read())

        log_info(f"[UPLOAD {session_id}] üìÅ File WAV nh·∫≠n: {file.filename} ‚Üí {temp_path}")

        # --------------------------------------------------------
        # 2) G·ª≠i file WAV v√†o pipeline WebRTC STT Processor
        # --------------------------------------------------------
        dm_processor = RTCStreamProcessor(log_callback=log_info)
        stream_gen = dm_processor.handle_rtc_session(
            record_file=Path(temp_path),
            session_id=session_id,
            api_key=api_key or INTERNAL_API_KEY,
        )
        logic_manager.api_key = api_key or INTERNAL_API_KEY
        dialog_manager.api_key = api_key or INTERNAL_API_KEY

        last_user_text = ""
        audio_chunks_binary = []
        final_text_data = {}

        # --------------------------------------------------------
        # 3) ƒê·ªçc k·∫øt qu·∫£ STT & audio t·ª´ pipeline
        # --------------------------------------------------------
        async for is_audio, data in stream_gen:

            # AUDIO STREAM
            if is_audio:
                audio_chunks_binary.append(
                    base64.b64decode(data) if isinstance(data, str) else data
                )
                continue

            # TEXT STREAM
            user_text = data.get("user_text", "").strip()
            last_user_text = user_text

            # ----------------------------------------------------
            # 4) PARSER ‚Üí convert JSON STT ‚Üí JSON NLU chu·∫©n
            # ----------------------------------------------------
            parser = STTLogParser(log_callback=log_info)
            nlu_json = parser.convert({
                "text_response": {"user_text": user_text}
            })

            # ----------------------------------------------------
            # 5) LogicManager ‚Üí x√°c ƒë·ªãnh action & intent
            # ----------------------------------------------------
            decision = logic_manager.handle_nlu_result(nlu_json)

            # ----------------------------------------------------
            # 6) DialogManager ‚Üí t·∫°o bot_text
            # ----------------------------------------------------
            final_response = dialog_manager.process_with_logic_manager(
                nlu_json=nlu_json,
                logic_manager=logic_manager
            )
            logic_manager.api_key = api_key
            dialog_manager.api_key = api_key

            bot_text = final_response.get("response_text") or final_response.get("text") or ""

            final_text_data = {
                "user_text": user_text,
                "bot_text": bot_text,
                "intent": decision.get("intent"),
                "action": decision.get("action"),
                "payment_url": decision.get("payment_url")
            }

        # --------------------------------------------------------
        # 7) GHI BOT AUDIO ‚Äî n·∫øu pipeline STT kh√¥ng tr·∫£ √¢m thanh
        # --------------------------------------------------------
        from pydub import AudioSegment

        tts_text = (
            f"B·∫°n v·ª´a n√≥i: {final_text_data.get('user_text', '')}. "
            f"C√¢u tr·∫£ l·ªùi c·ªßa t√¥i l√†: {final_text_data.get('bot_text', '')}."
        )

        mp3_path = os.path.join("temp", f"{session_id}_tts.mp3")
        wav_path = os.path.join("temp", f"{session_id}_output.wav")

        # TTS ‚Üí MP3
        gTTS(tts_text, lang="vi").save(mp3_path)

        # MP3 ‚Üí WAV chu·∫©n PCM16
        sound = AudioSegment.from_mp3(mp3_path)
        sound = sound.set_frame_rate(16000).set_channels(1).set_sample_width(2)
        sound.export(wav_path, format="wav")

        output_file = wav_path


        # --------------------------------------------------------
        # 8) Chu·∫©n b·ªã JSON tr·∫£ v·ªÅ
        # --------------------------------------------------------
        response = {
            "session_id": session_id,
            "user_text": final_text_data.get("user_text", ""),
            "bot_text": final_text_data.get("bot_text", ""),
            "intent": final_text_data.get("intent", ""),
            "action": final_text_data.get("action", ""),
            "payment_url": final_text_data.get("payment_url", None),
            "bot_audio_path": f"/audio_files/{Path(output_file).name}" if os.path.exists(output_file) else None,
        }

        log_info(f"[UPLOAD {session_id}] üéØ K·∫øt qu·∫£: {response['bot_text']}")

        return JSONResponse(response)

    except Exception as e:
        log_info(f"[UPLOAD] ‚ùå L·ªói x·ª≠ l√Ω file WAV: {e}")
        traceback.print_exc()
        return JSONResponse({"error": str(e)}, status_code=500)
# ============================================================
# STATIC ROUTES ‚Äî SERVE AUDIO FILES & STATIC HTML
# ============================================================
from fastapi.responses import FileResponse

@app.get("/audio_files/{filename}")
async def serve_audio_file(filename: str):
    file_path = os.path.join("temp", filename)
    return FileResponse(
        file_path,
        media_type="audio/wav",
        headers={"Accept-Ranges": "none"}   # üö´ NgƒÉn tr√¨nh duy·ªát g·ª≠i Range requests
    )

# Th∆∞ m·ª•c static ‚Üí ch·ª©a QR payment, HTML demo UI
app.mount("/static", StaticFiles(directory="static"), name="static")



# ============================================================
# MAIN ENTRY (CH·∫†Y B·∫∞NG PYTHON TR·ª∞C TI·∫æP)
# ============================================================
if __name__ == "__main__":
    import uvicorn
    log_info("üöÄ Backend WebRTC STT ƒëang kh·ªüi ƒë·ªông...")
    uvicorn.run(app, host="127.0.0.1", port=8000)
print("\n==== ROUTES ====")
for r in app.routes:
    print(r.path, type(r))
print("==== END ROUTES ====\n")
